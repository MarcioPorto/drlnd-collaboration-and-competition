# DRLND Project 3 Report (Collaboration and Competition)

## Introduction

For this project, I decided to work on solving the version of the Reacher environment with 20 agents. I chose to implement the DDPG algorithm, based on a [previous implementation](https://github.com/MarcioPorto/deep-reinforcement-learning/tree/master/ddpg-pendulum) for the `Pendulum` Gym environment. The decision to use DDPG was based on the fact that it extends the power of the popular DQN algorithm to environments with continuous action spaces, such as this. However, there are many other policy-based algorithms that might work well for solving this kind of environment, including: TRPO, PPO, and A3C.

## Learning Algorithm

## Training and Results

## Future Work Ideas


<!-- 
# DRLND Project 2 Report (Reacher)

## Introduction

This submission uses a refactored and simplified version of the MADDPG implementation provided in the Udacity workspace for the Physical Deception problem.

## Algorithm and Implementation

Spend a great deal of time cleaning up the code used in the Physical Deception lab. I decided to use a class organization...

## Future Work Ideas

- Run the environment in parallel.
- Try a different multi-agent algorithm like MA-PPO.

## Notes

- What exactly is obs and obs_full?
- Used ReplayBuffer from project 2 -->
