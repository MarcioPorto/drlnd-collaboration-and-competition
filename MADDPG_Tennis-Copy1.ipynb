{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "env = UnityEnvironment(seed=RANDOM_SEED, file_name=\"Tennis.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=512, fc2_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(RANDOM_SEED)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.tanh(self.fc3(x))\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, fcs1_units=512, fc2_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(RANDOM_SEED)\n",
    "        self.fcs1 = nn.Linear((state_size + action_size) * NUM_AGENTS, fcs1_units)\n",
    "        self.fc2 = nn.Linear(fcs1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fcs1.weight.data.uniform_(*hidden_init(self.fcs1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state, action) pairs -> Q-values.\"\"\"\n",
    "        # Concatenates states and actions as the input of the first layer\n",
    "        # DDPG does this before the second layer\n",
    "        xs = torch.cat((state, action), dim=1)\n",
    "        x = F.relu(self.fcs1(xs))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, mu=0.0, theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(RANDOM_SEED)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.array([np.random.normal(loc=0, scale=1) for _ in range(len(x))])\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(RANDOM_SEED)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class DDPGAgent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, agent_id):\n",
    "        \"\"\"Initialize a DDPGAgent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            agent_id (int): identifier for this agent\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(RANDOM_SEED)\n",
    "        self.agent_id = agent_id\n",
    "\n",
    "        self.actor_local = Actor(state_size, action_size).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        self.critic_local = Critic(state_size, action_size).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Make sure that the target-local model pairs are initialized to the \n",
    "        # same weights\n",
    "        self.hard_update(self.actor_local, self.actor_target)\n",
    "        self.hard_update(self.critic_local, self.critic_target)\n",
    "        \n",
    "        self.noise = OUNoise(action_size)\n",
    "        \n",
    "        self.noise_amplification = NOISE_AMPLIFICATION\n",
    "        self.noise_amplification_decay = NOISE_AMPLIFICATION_DECAY\n",
    "        \n",
    "        self._print_network()\n",
    "\n",
    "    def act(self, state, add_noise=False):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        \n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        if add_noise:\n",
    "            action += self.noise.sample()\n",
    "            self._decay_noise_amplification()\n",
    "        \n",
    "        return np.clip(action, -1, 1)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets the OU Noise for this agent.\"\"\"\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def learn(self, experiences, next_actions, actions_pred):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(next_state) -> action\n",
    "            critic_target(next_state, next_action) -> Q-value\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            next_actions (list): next actions computed from each agent\n",
    "            actions_pred (list): prediction for actions for current states from each agent\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        agent_id_tensor = torch.tensor([self.agent_id - 1]).to(device)\n",
    "\n",
    "        ### Update critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        Q_targets_next = self.critic_target(next_states, next_actions)        \n",
    "        Q_targets = rewards.index_select(1, agent_id_tensor) + (GAMMA * Q_targets_next * (1 - dones.index_select(1, agent_id_tensor)))\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        # Minimize the loss\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        ### Update actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        # Minimize the loss\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        ### Update target networks\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)\n",
    "        \n",
    "    def hard_update(self, local_model, target_model):\n",
    "        \"\"\"Hard update model parameters.\n",
    "        θ_target = θ_local\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(local_param.data)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ * θ_local + (1 - τ) * θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "    \n",
    "    def _print_network(self):\n",
    "        \"\"\"Helper to print network architecture for this agent's actors and critics.\"\"\"\n",
    "        print(\"Agent #{}\".format(self.agent_id))\n",
    "        print(\"Actor (Local):\")\n",
    "        print(self.actor_local)\n",
    "        print(\"Actor (Target):\")\n",
    "        print(self.actor_target)\n",
    "        print(\"Critic (Local):\")\n",
    "        print(self.critic_local)\n",
    "        print(\"Critic (Target):\")\n",
    "        print(self.critic_target)\n",
    "        if self.agent_id != NUM_AGENTS:\n",
    "            print(\"_______________________________________________________________\")\n",
    "            \n",
    "    def _decay_noise_amplification(self):\n",
    "        \"\"\"Helper for decaying exploration noise amplification.\"\"\"\n",
    "        self.noise_amplification *= self.noise_amplification_decay\n",
    "\n",
    "\n",
    "class MADDPGAgent():\n",
    "    \"\"\"Wrapper class managing different agents in the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, num_agents, state_size, action_size):\n",
    "        \"\"\"Initialize a MADDPGAgent wrapper.\n",
    "        Params\n",
    "        ======\n",
    "            num_agents (int): the number of agents in the environment\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "        \"\"\"\n",
    "        self.num_agents = num_agents\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.agents = [DDPGAgent(state_size, action_size, i+1) for i in range(num_agents)]\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "        \n",
    "        # Will help to decide when to update the model weights\n",
    "        self.t_step = 0\n",
    "        \n",
    "        # Directory where to save the model\n",
    "        self.model_dir = os.getcwd() + \"/saved_models\"\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Resets OU Noise for each agent.\"\"\"\n",
    "        for agent in self.agents:\n",
    "            agent.reset()\n",
    "            \n",
    "    def act(self, observations, add_noise=False):\n",
    "        \"\"\"Picks an action for each agent given their individual observations \n",
    "        and the current policy.\"\"\"\n",
    "        actions = []\n",
    "        for agent, observation in zip(self.agents, observations):\n",
    "            action = agent.act(observation, add_noise=add_noise)\n",
    "            actions.append(action)\n",
    "        return np.array(actions)\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        states = states.reshape(1, -1)\n",
    "        actions = actions.reshape(1, -1)\n",
    "        next_states = next_states.reshape(1, -1)\n",
    "        \n",
    "        self.memory.add(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "        \n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE and self.t_step == 0:\n",
    "            for a_i, agent in enumerate(self.agents):\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, a_i)\n",
    "            \n",
    "    def learn(self, experiences, agent_number):\n",
    "        \"\"\"Helper to pick actions from each agent for the `experiences` tuple that \n",
    "        will be used to update the weights to agent with ID = `agent_number`.\n",
    "        Each observation in the `experiences` tuple contains observations from each \n",
    "        agent, so before using the tuple of update the weights of an agent, we need \n",
    "        all agents to contribute in generating `next_actions` and `actions_pred`. \n",
    "        This happens because the critic will take as its input the combined \n",
    "        observations and actions from all agents.\"\"\"\n",
    "        next_actions = []\n",
    "        actions_pred = []\n",
    "        states, _, _, next_states, _ = experiences\n",
    "        \n",
    "        next_states = next_states.reshape(-1, self.num_agents, self.state_size)\n",
    "        states = states.reshape(-1, self.num_agents, self.state_size)\n",
    "        \n",
    "        for a_i, agent in enumerate(self.agents):\n",
    "            agent_id_tensor = self._get_agent_number(a_i)\n",
    "            \n",
    "            state = states.index_select(1, agent_id_tensor).squeeze(1)\n",
    "            next_state = next_states.index_select(1, agent_id_tensor).squeeze(1)\n",
    "            \n",
    "            next_actions.append(agent.actor_target(next_state))\n",
    "            actions_pred.append(agent.actor_local(state))\n",
    "            \n",
    "        next_actions = torch.cat(next_actions, dim=1).to(device)\n",
    "        actions_pred = torch.cat(actions_pred, dim=1).to(device)\n",
    "        \n",
    "        agent = self.agents[agent_number]\n",
    "        agent.learn(experiences, next_actions, actions_pred)\n",
    "            \n",
    "    def save_model(self):\n",
    "        \"\"\"Saves model weights to file.\"\"\"\n",
    "        for i in range(self.num_agents):\n",
    "            torch.save(\n",
    "                self.agents[i].actor_local.state_dict(), \n",
    "                os.path.join(self.model_dir, 'actor_params.pth')\n",
    "            )\n",
    "            torch.save(\n",
    "                self.agents[i].actor_optimizer.state_dict(), \n",
    "                os.path.join(self.model_dir, 'actor_optim_params.pth')\n",
    "            )\n",
    "            torch.save(\n",
    "                self.agents[i].critic_local.state_dict(), \n",
    "                os.path.join(self.model_dir, 'critic_params.pth')\n",
    "            )\n",
    "            torch.save(\n",
    "                self.agents[i].critic_optimizer.state_dict(), \n",
    "                os.path.join(self.model_dir, 'critic_optim_params.pth')\n",
    "            )\n",
    "    \n",
    "    def load_model(self):\n",
    "        \"\"\"Loads weights from saved model.\"\"\"\n",
    "        for i in range(self.num_agents):\n",
    "            self.agents[i].actor_local.load_state_dict(\n",
    "                torch.load(os.path.join(self.model_dir, 'actor_params_{}.pth'.format(i)))\n",
    "            )\n",
    "            self.agents[i].actor_optimizer.load_state_dict(\n",
    "                torch.load(os.path.join(self.model_dir, 'actor_optim_params_{}.pth'.format(i)))\n",
    "            )\n",
    "            self.agents[i].critic_local.load_state_dict(\n",
    "                torch.load(os.path.join(self.model_dir, 'critic_params_{}.pth'.format(i)))\n",
    "            )\n",
    "            self.agents[i].critic_optimizer.load_state_dict(\n",
    "                torch.load(os.path.join(self.model_dir, 'critic_optim_params_{}.pth'.format(i)))\n",
    "            )\n",
    "    \n",
    "    def _get_agent_number(self, i):\n",
    "        \"\"\"Helper to get an agent's number as a Torch tensor.\"\"\"\n",
    "        return torch.tensor([i]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e6)  # replay buffer size\n",
    "BATCH_SIZE = 512        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 5e-2              # for soft update of target parameters\n",
    "LR_ACTOR = 5e-4         # learning rate of the actor \n",
    "LR_CRITIC = 5e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0.0      # L2 weight decay\n",
    "UPDATE_EVERY = 2\n",
    "NUM_AGENTS = num_agents\n",
    "STATE_SIZE = int(state_size)  # this environment has 3 time frames stacked together as its state\n",
    "ACTION_SIZE = action_size\n",
    "NOISE_AMPLIFICATION = 1\n",
    "NOISE_AMPLIFICATION_DECAY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import progressbar as pb\n",
    "\n",
    "\n",
    "def train(maddpg, n_episodes=1000, max_t=1000, save_every=50):\n",
    "    widget = [\n",
    "        \"Episode: \", pb.Counter(), '/' , str(n_episodes), ' ', \n",
    "        pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ', \n",
    "        'Rolling Average: ', pb.FormatLabel('')\n",
    "    ]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=n_episodes).start()\n",
    "\n",
    "    solved = False\n",
    "    scores_total = []\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    rolling_score_averages = []\n",
    "    last_best_score = 0.0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        current_average = 0.0 if i_episode == 1 else rolling_score_averages[-1]\n",
    "        widget[12] = pb.FormatLabel(str(current_average)[:6])\n",
    "        timer.update(i_episode)\n",
    "            \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations[:, -STATE_SIZE:]\n",
    "        scores = np.zeros(num_agents)\n",
    "        maddpg.reset()\n",
    "        \n",
    "        # for t in range(max_t):\n",
    "        while True:\n",
    "            actions = maddpg.act(states)\n",
    "            \n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations[:, -STATE_SIZE:]\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            \n",
    "            maddpg.step(states, actions, rewards, next_states, dones)\n",
    "            \n",
    "            scores += rewards\n",
    "            states = next_states\n",
    "            \n",
    "            if np.any(dones):\n",
    "                break\n",
    "        \n",
    "        max_episode_score = np.max(scores)\n",
    "        \n",
    "        scores_deque.append(max_episode_score)\n",
    "        scores_total.append(max_episode_score)\n",
    "\n",
    "        average_score = np.mean(scores_deque)\n",
    "        rolling_score_averages.append(average_score)\n",
    "        \n",
    "        if average_score >= 0.5 and not solved:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(\n",
    "                i_episode, average_score\n",
    "            ))\n",
    "            solved = True\n",
    "            maddpg.save_model()\n",
    "            last_best_score = average_score\n",
    "        \n",
    "        if i_episode % save_every == 0 and solved:\n",
    "            # Only save these weights if they are better than the ones previously saved\n",
    "            if average_score > last_best_score:\n",
    "                last_best_score = average_score\n",
    "                maddpg.save_model()\n",
    "\n",
    "    return scores_total, rolling_score_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent #1\n",
      "Actor (Local):\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=24, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "Actor (Target):\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=24, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "Critic (Local):\n",
      "Critic(\n",
      "  (fcs1): Linear(in_features=52, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Critic (Target):\n",
      "Critic(\n",
      "  (fcs1): Linear(in_features=52, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "_______________________________________________________________\n",
      "Agent #2\n",
      "Actor (Local):\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=24, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "Actor (Target):\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=24, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "Critic (Local):\n",
      "Critic(\n",
      "  (fcs1): Linear(in_features=52, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n",
      "Critic (Target):\n",
      "Critic(\n",
      "  (fcs1): Linear(in_features=52, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "maddpg = MADDPGAgent(NUM_AGENTS, STATE_SIZE, ACTION_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode: 700/2000  35% ETA:  1:29:17 |-----           | Rolling Average: 0.5011\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment solved in 699 episodes!\tAverage Score: 0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode: 715/2000  35% ETA:  1:31:10 ||||||           | Rolling Average: 0.4929\r"
     ]
    }
   ],
   "source": [
    "scores_total, rolling_score_averages = train(maddpg, n_episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(scores, rolling_score_averages):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    plt.plot(np.arange(1, len(scores) + 1), scores, label=\"Max Score\")\n",
    "    plt.plot(np.arange(1, len(rolling_score_averages) + 1), rolling_score_averages, label=\"Rolling Average\")\n",
    "    plt.axhline(y=0.5, color=\"r\", linestyle=\"-\", label=\"Environment Solved\")\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Episode #\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe1e4894940>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(scores_total, rolling_score_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
